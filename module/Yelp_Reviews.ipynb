{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yelp_Reviews.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReQCim3cHpv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install Java, Spark, and Findspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XEIL7g-Hqv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Yelp_NLP\").getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll_1CiQZHsXb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "85f08d5b-6463-40d3-f123-e32a02b4fd6c"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/yelp_reviews.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"yelp_reviews.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+\n",
            "|   class|                text|\n",
            "+--------+--------------------+\n",
            "|positive|Wow... Loved this...|\n",
            "|negative|  Crust is not good.|\n",
            "|negative|Not tasty and the...|\n",
            "|positive|Stopped by during...|\n",
            "|positive|The selection on ...|\n",
            "|negative|Now I am getting ...|\n",
            "|negative|Honeslty it didn'...|\n",
            "|negative|The potatoes were...|\n",
            "|positive|The fries were gr...|\n",
            "|positive|      A great touch.|\n",
            "|positive|Service was very ...|\n",
            "|negative|  Would not go back.|\n",
            "|negative|The cashier had n...|\n",
            "|positive|I tried the Cape ...|\n",
            "|negative|I was disgusted b...|\n",
            "|negative|I was shocked bec...|\n",
            "|positive| Highly recommended.|\n",
            "|negative|Waitress was a li...|\n",
            "|negative|This place is not...|\n",
            "|negative|did not like at all.|\n",
            "+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smwwYj4DHv8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import functions\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6JH5PDcHwh7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "351d6be8-d110-4c6b-a84a-aeed1ebe2bcd"
      },
      "source": [
        "from pyspark.sql.functions import length\n",
        "# Create a length column to be used as a future feature\n",
        "data_df = df.withColumn('length', length(df['text']))\n",
        "data_df.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+------+\n",
            "|   class|                text|length|\n",
            "+--------+--------------------+------+\n",
            "|positive|Wow... Loved this...|    24|\n",
            "|negative|  Crust is not good.|    18|\n",
            "|negative|Not tasty and the...|    41|\n",
            "|positive|Stopped by during...|    87|\n",
            "|positive|The selection on ...|    59|\n",
            "|negative|Now I am getting ...|    46|\n",
            "|negative|Honeslty it didn'...|    37|\n",
            "|negative|The potatoes were...|   111|\n",
            "|positive|The fries were gr...|    25|\n",
            "|positive|      A great touch.|    14|\n",
            "|positive|Service was very ...|    24|\n",
            "|negative|  Would not go back.|    18|\n",
            "|negative|The cashier had n...|    99|\n",
            "|positive|I tried the Cape ...|    59|\n",
            "|negative|I was disgusted b...|    62|\n",
            "|negative|I was shocked bec...|    50|\n",
            "|positive| Highly recommended.|    19|\n",
            "|negative|Waitress was a li...|    38|\n",
            "|negative|This place is not...|    51|\n",
            "|negative|did not like at all.|    20|\n",
            "+--------+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XADY1BUmH5tX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create all the features to the data set\n",
        "pos_neg_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
        "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
        "hashingTF = HashingTF(inputCol=\"token_text\", outputCol='hash_token')\n",
        "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2se6T9RJH_eI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vector\n",
        "# Create feature vectors\n",
        "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-SjPRDLIHce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create and run a data processing Pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-eY-1BbIILq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit and transform the pipeline\n",
        "cleaner = data_prep_pipeline.fit(data_df)\n",
        "cleaned = cleaner.transform(data_df)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsObmwYEI3AL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Break data down into a training set and a testing set\n",
        "training, testing = cleaned.randomSplit([0.7, 0.3], 21)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn8Twx9GJnZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "# Create a Naive Bayes model and fit training data\n",
        "nb = NaiveBayes()\n",
        "predictor = nb.fit(training)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjRZaUygJu_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "8cd8133e-83b9-438d-9d9c-369d20bd4be3"
      },
      "source": [
        "test_results = predictor.transform(testing)\n",
        "test_results.show(5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|   class|                text|length|label|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|negative| \"As for the \"\"mains|    19|  1.0|[\"as, for, the, \"...|      [\"as, \"\"mains]|(262144,[16332,10...|(262144,[16332,10...|(262145,[16332,10...|[-235.05580770267...|[0.03900923601832...|       1.0|\n",
            "|negative|\"I don't know wha...|    85|  1.0|[\"i, don't, know,...|[\"i, know, big, d...|(262144,[8478,158...|(262144,[8478,158...|(262145,[8478,158...|[-876.28450835078...|[5.82709985215757...|       1.0|\n",
            "|negative|\"like the other r...|    82|  1.0|[\"like, the, othe...|[\"like, reviewer,...|(262144,[22808,61...|(262144,[22808,61...|(262145,[22808,61...|[-930.20054206924...|[5.67229347354298...|       1.0|\n",
            "|negative| \"not even a \"\"hello|    19|  1.0|[\"not, even, a, \"...|[\"not, even, \"\"he...|(262144,[174966,2...|(262144,[174966,2...|(262145,[174966,2...|[-259.46346332024...|[0.19845430741939...|       1.0|\n",
            "|negative|              #NAME?|     6|  1.0|            [#name?]|            [#name?]|(262144,[75443],[...|(262144,[75443],[...|(262145,[75443,26...|[-70.880216880965...|[0.93675076661110...|       0.0|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81PZKW1PKH6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "505eb36f-5938-4b22-d30a-daef2035861f"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "acc_eval = MulticlassClassificationEvaluator()\n",
        "acc = acc_eval.evaluate(test_results)\n",
        "print(\"Accuracy of Model at predicting reviews was %f:\" % acc)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Model at predicting reviews was 0.718984:\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}